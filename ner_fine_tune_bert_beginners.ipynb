{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü§î **Fine-tunning a NER model with BERT for Beginners**\n",
        "---\n",
        "\n",
        "Are you a beginner? Do you want to learn, but don't know where to start? In this tutorial, you will learn to fine-tune a pre-trained BERT model for Named Entity Recognition. It will walk you through the following steps:\n",
        "\n",
        "- üöÄ Load your training dataset into Argilla and explore it using its tools.\n",
        "- ‚è≥ Preprocess the data to generate the other inputs required by the model, and put them in a format that the model expects.\n",
        "- üîç Download the BERT model and start to fine-tune it.\n",
        "- üß™ Perform your own tests!"
      ],
      "metadata": {
        "id": "CCsbozBWRESe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1XUA1RS8p_ARxvtQGTvnoeuDF-JDBfBxv\"\n",
        "  alt=\"NER\" \n",
        "  width=\"700\" \n",
        "  height=\"400\" \n",
        "  style=\"display: block; margin: 0 auto\" />"
      ],
      "metadata": {
        "id": "10I7EROxWPqy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs0Vhje60q4p"
      },
      "source": [
        "## Introduction\n",
        "---\n",
        "\n",
        "Our goal is to show from a trainig dataset how to fine-tune a **tiny BERT model** in order to identify **NER** tags.\n",
        "\n",
        "For this purpose, we will first connect to Argilla and log our [dataset](https://huggingface.co/datasets/argilla/spacy_sm_wnut17), so that we can analyse it in a more visual way.\n",
        "\n",
        ">üí° **Tip:** If you want to try with a different dataset than the one in this tutorial, but it's not yet annotated, Argilla has several tutorials on how to do it [manually](https://docs.argilla.io/en/latest/guides/how_to.html#1.-Manual-labeling) or [automatically](https://docs.argilla.io/en/latest/tutorials/notebooks/labelling-tokenclassification-spacy-pretrained.html#Appendix:-Log-datasets-to-the-Hugging-Face-Hub).\n",
        "\n",
        "\n",
        "Next, we will preprocess our dataset and fine-tune the model. Here we will be using [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert), to make it easier to understand it and start *playing* with the parameters easily. However, there are still plenty of similar ones to [discover](https://huggingface.co/docs/transformers/index#bigtable).\n",
        "\n",
        "‚ú®Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnKU83tp0q4q"
      },
      "source": [
        "## Running Argilla\n",
        "---\n",
        "\n",
        "For this tutorial, you will need to have an Argilla server running. There are two main options for deploying and running Argilla:\n",
        "\n",
        "1. [Deploy Argilla on Hugging Face Spaces](https://huggingface.co/docs/hub/spaces-sdks-docker-argilla): This is the fastest option and the recommended choice for connecting to external notebooks (e.g., Google Colab) if you have an account on Hugging Face.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <a href=\"https://huggingface.co/new-space?template=argilla/argilla-template-space\">\n",
        "    <img src=\"https://huggingface.co/datasets/huggingface/badges/raw/main/deploy-to-spaces-lg.svg\" alt=\"deploy on spaces\">\n",
        "  </a>\n",
        "</p>\n",
        "\n",
        "2. [Launch Argilla using Argilla's quickstart Docker image](../../getting_started/quickstart.ipynb): This is the recommended option if you want Argilla running on your local machine. Note that this option will only let you run the tutorial locally and not with an external notebook service.\n",
        "\n",
        "For more information on deployment options, please check the Deployment section of the documentation.\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "\n",
        "ü§Ø **Tip**\n",
        "\n",
        " This tutorial is a Jupyter Notebook. There are two options to run it:\n",
        "- Use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference.\n",
        "- Download the .ipynb file by clicking on the View source link at the top of the page. This option allows you to download the notebook and run it on your local machine or on a Jupyter notebook tool of your choice.\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "---\n",
        "\n",
        "For this tutorial, you'll need to install the Argilla client and a few third party libraries using `pip`:"
      ],
      "metadata": {
        "id": "KBaIylKyh1mF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80Xv9vqx0q4r"
      },
      "outputs": [],
      "source": [
        "%pip install \"argilla[server]==1.5.0\" -qqq\n",
        "%pip install datasets\n",
        "%pip install transformers\n",
        "%pip install evaluate\n",
        "%pip install seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhoCuW7y0q4s"
      },
      "source": [
        "Let's import the Argilla module for reading and writing data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ5IcIyG0q4t"
      },
      "outputs": [],
      "source": [
        "import argilla as rg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO241Tyq0q4t"
      },
      "source": [
        "If you are running Argilla using the Docker quickstart image or Hugging Face Spaces, you need to init the Argilla client with the `URL` and `API_KEY`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Olsh_SY0q4t"
      },
      "outputs": [],
      "source": [
        "# Replace api_url with the url to your HF Spaces URL if using Spaces (in the three dots in the right-hand corner choose \"Embed this Space\")\n",
        "# Replace api_key if you configured a custom API key\n",
        "\n",
        "rg.init(\n",
        "    api_url=\"http://localhost:6900\", \n",
        "    api_key=\"team.apikey\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKgEhWk70q4u"
      },
      "source": [
        "Finally, let's include the imports we need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW09SGfn0q4u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import evaluate\n",
        "import transformers\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "from datasets import load_dataset, ClassLabel, Sequence\n",
        "from argilla.metrics.token_classification import top_k_mentions\n",
        "from argilla.metrics.token_classification.metrics import Annotations\n",
        "from IPython.display import display, HTML\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification, pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Exploring our dataset\n",
        "---"
      ],
      "metadata": {
        "id": "n40-m2PXmcM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will load the train of our dataset from HuggingFace in order to explore it using ``load_dataset``. And, as we can see, it has 119 entries and two columns: one with the sequence of tokens and the other with the sequence of NER tags."
      ],
      "metadata": {
        "id": "6KIAb_u4v8vL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"argilla/spacy_sm_wnut17\", split = \"train\")"
      ],
      "metadata": {
        "id": "s_e7D8paZFDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "L9_dS8-SvAPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will use the following code taking advantage of the ``DatasetDict`` option ``Features`` to convert it to the format required by Argilla in order to log it.\n",
        "\n",
        "The three elements that our data must have for Token Classifications are the following:\n",
        "\n",
        "* **text**: the complete string.\n",
        "* **tokens**: the sequence of tokens.\n",
        "* **annotation**: a tuple formed by the tag, the start position and the end position.\n",
        "\n",
        "> ‚ö†Ô∏è **Be careful:** Each execution will upload and add your annotations again without being overwritten."
      ],
      "metadata": {
        "id": "ITgB-mzZz3vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_entities(record):\n",
        "    entities = []\n",
        "    counter = 0\n",
        "    for i in range(len(record[\"ner_tags\"])):\n",
        "        entity = (\n",
        "            dataset.features[\"ner_tags\"].feature.names[record[\"ner_tags\"][i]],\n",
        "            counter,\n",
        "            counter + len(record[\"tokens\"][i]),\n",
        "        )\n",
        "        entities.append(entity)\n",
        "        counter += len(record[\"tokens\"][i]) + 1\n",
        "    return entities"
      ],
      "metadata": {
        "id": "1PST31jNZoXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a loop to iterate over each row of your dataset and add the text, tokens, and tuple\n",
        "records = [\n",
        "    rg.TokenClassificationRecord(\n",
        "        text=\" \".join(row[\"tokens\"]),\n",
        "        tokens=row[\"tokens\"],\n",
        "        annotation=parse_entities(row),\n",
        "    )\n",
        "    for row in dataset\n",
        "]\n",
        "\n",
        "# Log the records with the name of your choice\n",
        "rg.log(records, \"spacy_sm_wnut17\")"
      ],
      "metadata": {
        "id": "WsKBFFAiYv3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now you will be able to check your annotations in a much more visual way and even edit them if necessary.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=191wpFz5MsQtiJNFeH9IXizr65qtGE3MP\"\n",
        "  alt=\"NER\" \n",
        "  width=\"1500\" \n",
        "  height=\"400\" \n",
        "  style=\"display: block; margin: 0 auto\" />\n",
        "\n",
        "In addition, **Argilla** also has more options, e.g. to extract [metrics](https://docs.argilla.io/en/latest/reference/python/python_metrics.html) such as the ones shown below.\n",
        "\n"
      ],
      "metadata": {
        "id": "3Ydx5se82s_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the dataset from Argilla and visualize the data\n",
        "top_k_mentions(\n",
        "    name=\"spacy_sm_wnut17\", k=30, threshold=2, compute_for=Annotations\n",
        ").visualize()"
      ],
      "metadata": {
        "id": "qyYxVdygh5Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚è≥ Preprocessing the data\n",
        "---"
      ],
      "metadata": {
        "id": "ypCRJhX35ltp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will **pre-process our data** in the required format so that the model can work with it. In our case, we will reload them from HuggingFace, as in Argilla we only loaded the train set, however, this is also possible.\n",
        "\n",
        "The following code would allow us to prepare our data using Argilla, this is especially useful for manual annotations, as it adds **B-** (beggining) or **I-** (inside) to our NER tags automatically depending on their position.\n",
        "\n",
        "```python\n",
        "dataset = rg.load(\"dataset_name\").prepare_for_training()\n",
        "\n",
        "dataset = dataset.train_test_split()\n",
        "```\n",
        "\n",
        "> ü§Ø **Tip:** In our case, we are working with a very small dataset that is divided into train and test. However, you may are using another dataset that already have the ``validation`` partition, or even if it is larger, you can create this partition yourself with the following code:\n",
        "\n",
        "```python\n",
        "dataset['train'], dataset['validation'] = dataset['train'].train_test_split(.1).values()\n",
        "```\n",
        "So, let's continue!"
      ],
      "metadata": {
        "id": "ffpxCD1M8mFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"argilla/spacy_sm_wnut17\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "pl1MwBhxkQEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time to **tokenize**! Although it seems like this is already done, each token still needs to be converted into a vector (ID) that the model can read from its pretrained vocabulary. To do this, we will use ``AutoTokenizer.from_pretrained`` and the FastTokenizer ``distilbert-base-uncased``."
      ],
      "metadata": {
        "id": "jeAuJM2XAX11"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRzo_3NldUew"
      },
      "outputs": [],
      "source": [
        "# Example of original tokens\n",
        "example = dataset[\"train\"][0]\n",
        "print(example[\"tokens\"])\n",
        "\n",
        "# Example after executing the AutoTokenizer\n",
        "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we now ran into a new problem. Since it tokenises according to a pre-trained vocabulary, this will create new subdivisions in some words (e.g. \"'s\" in \"'\" and \"s\"). In addition, it adds two new tags [CLS] and [SEP]. Therefore, we have to realign the IDs of our words with the corresponding NER tags, thanks to the ``word-ids`` method."
      ],
      "metadata": {
        "id": "Qzw7BRWYGiZn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc0BSBLIIrJQ"
      },
      "outputs": [],
      "source": [
        "label_all_tokens = True\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Fine-tunning the model\n",
        "---\n",
        "We should now start preparing the parameters of our model, i.e. we should start fine-tuning."
      ],
      "metadata": {
        "id": "4HttMIC1KWWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Model*\n",
        "\n",
        "First we will download our pretrained model with ``AutoModelForTokenClassification`` and we will indicate the name of the chosen model, the number of tags and we will indicate the correspondences between their IDs and their names.\n",
        "\n",
        "In addition, we will also set our ``DataCollator`` to form a batch by using our processed examples as input. In this case, we will be using ``DataCollatorForTokenClassification``."
      ],
      "metadata": {
        "id": "UauakSYrMs3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary with the ids and the relevant label.\n",
        "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "# Download the model.\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(label_list), id2label=id2label, label2id=label2id)\n",
        "\n",
        "# Set the DataCollator\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ],
      "metadata": {
        "id": "M9CW8u2mNKmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Training arguments*\n",
        "The ``TrainingArguments`` class will include the [parameters](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) to customise our training.\n",
        "\n",
        "> üí° **Tip:** If you are using HuggingFace it may be easier for you to save your model there directly. To do so, use the following code and add the following parameters to TrainingArguments.\n",
        "\n",
        "```python\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "# Add the following parameter\n",
        "training_args = TrainingArguments(\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "```\n",
        "> üïπÔ∏è **Let's play:** What is the best accuracy you can get?"
      ],
      "metadata": {
        "id": "mkTNDfJWN0PH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"ner-recognition\",\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=20,\n",
        "    weight_decay=0.05,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    optim=\"adamw_torch\",\n",
        "    logging_steps = 50\n",
        ")"
      ],
      "metadata": {
        "id": "O9lz1LbVN3OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Metrics*\n",
        "\n",
        "To know how our training has gone, of course, we must use metrics. Therefore, we will use ``Seqeval`` and a function that computes precision, recall, F1 and accuracy from the actual and predicted tags."
      ],
      "metadata": {
        "id": "7SFjM6KGN3s3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Sqeval.\n",
        "metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "# Create the list with the tags.\n",
        "labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
        "\n",
        "# Function to compute precision, recall, F1 and accuracy.\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "Jr559HdeN6Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Time to train*\n",
        "\n",
        "As the name of this section suggests, the time has come to bring all the previous elements together and start training with ``Trainer``."
      ],
      "metadata": {
        "id": "2idJm-i1N6nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train.\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "FT-PSxj7LLKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `evaluate` method will allow you to evaluate again on the validation set or on another dataset (e.g. if you have train, validation and test)."
      ],
      "metadata": {
        "id": "ZCCiyS9llFq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "tsX1Z9r_PjT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> üîÆ **Try to predict**\n",
        "\n",
        "> When you have created your model and are happy with it, test it yourself with your own text.\n",
        "\n",
        "```python\n",
        "# Replace this with the directory where it was saved\n",
        "model_checkpoint = \"your-path\"\n",
        "token_classifier = pipeline(\"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\")\n",
        "token_classifier(\"I heard Madrid is wonderful in spring.\")\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "0NYZJgRORyjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù‚úîÔ∏è Summary\n",
        "\n",
        "In this tutorial, we have learned how to upload our training dataset to Argilla in order to visualise the data it contains and the NER tags it uses and how to fine-tune a BERT model for NER recognition using ``transformers``. This can be very useful to learn the basics of BERT pre-models and, from there, to develop your skills further and try out different ones that may give better results.\n",
        "\n",
        "üí™Cheers!"
      ],
      "metadata": {
        "id": "4IDnpIN5ZE6u"
      }
    }
  ]
}